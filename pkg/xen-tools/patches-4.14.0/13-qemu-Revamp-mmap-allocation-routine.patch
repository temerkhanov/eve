From 3ad5ce46a00385fc59adb5ba9f7c4b358e346bbf Mon Sep 17 00:00:00 2001
From: Sergey Temerkhanov <s.temerkhanov@gmail.com>
Date: Sun, 20 Sep 2020 02:44:45 +0300
Subject: [PATCH 2/2] qemu: Revamp mmap allocation routine

Do not double-map the memory addresses

Signed-off-by: Sergey Temerkhanov <s.temerkhanov@gmail.com>
---
 util/mmap-alloc.c | 42 +++++++++++++++---------------------------
 1 file changed, 15 insertions(+), 27 deletions(-)

diff --git a/util/mmap-alloc.c b/util/mmap-alloc.c
index 27dcccd8ec..7ff2883e42 100644
--- a/tools/qemu-xen/util/mmap-alloc.c
+++ b/tools/qemu-xen/util/mmap-alloc.c
@@ -89,19 +89,18 @@ void *qemu_ram_mmap(int fd,
                     bool is_pmem)
 {
     int flags;
+    int mapfd;
     int map_sync_flags = 0;
-    int guardfd;
     size_t offset;
     size_t pagesize;
     size_t total;
-    void *guardptr;
     void *ptr;
+    int ret;
 
     /*
      * Note: this always allocates at least one extra page of virtual address
      * space, even if size is already aligned.
      */
-    total = size + align;
 
 #if defined(__powerpc64__) && defined(__linux__)
     /* On ppc64 mappings in the same segment (aka slice) must share the same
@@ -115,39 +114,31 @@ void *qemu_ram_mmap(int fd,
     flags = MAP_PRIVATE;
     pagesize = qemu_fd_getpagesize(fd);
     if (fd == -1 || pagesize == qemu_real_host_page_size) {
-        guardfd = -1;
+        mapfd = -1;
         flags |= MAP_ANONYMOUS;
     } else {
-        guardfd = fd;
+        mapfd = fd;
         flags |= MAP_NORESERVE;
     }
 #else
-    guardfd = -1;
+    mapfd = -1;
     pagesize = qemu_real_host_page_size;
     flags = MAP_PRIVATE | MAP_ANONYMOUS;
 #endif
 
-    guardptr = mmap(0, total, PROT_NONE, flags, guardfd, 0);
-
-    if (guardptr == MAP_FAILED) {
-        return MAP_FAILED;
-    }
-
     assert(is_power_of_2(align));
     /* Always align to host page size */
     assert(align >= pagesize);
 
-    flags = MAP_FIXED;
     flags |= fd == -1 ? MAP_ANONYMOUS : 0;
     flags |= shared ? MAP_SHARED : MAP_PRIVATE;
     if (shared && is_pmem) {
         map_sync_flags = MAP_SYNC | MAP_SHARED_VALIDATE;
     }
 
-    offset = QEMU_ALIGN_UP((uintptr_t)guardptr, align) - (uintptr_t)guardptr;
+    total = size + align + pagesize;
 
-    ptr = mmap(guardptr + offset, size, PROT_READ | PROT_WRITE,
-               flags | map_sync_flags, fd, 0);
+    ptr = mmap(0, total, PROT_READ | PROT_WRITE, flags | map_sync_flags, mapfd, 0);
 
     if (ptr == MAP_FAILED && map_sync_flags) {
         if (errno == ENOTSUP) {
@@ -171,29 +162,26 @@ void *qemu_ram_mmap(int fd,
          * if map failed with MAP_SHARED_VALIDATE | MAP_SYNC,
          * we will remove these flags to handle compatibility.
          */
-        ptr = mmap(guardptr + offset, size, PROT_READ | PROT_WRITE,
-                   flags, fd, 0);
+        ptr = mmap(0, size, PROT_READ | PROT_WRITE, flags, mapfd, 0);
     }
 
     if (ptr == MAP_FAILED) {
-        munmap(guardptr, total);
         return MAP_FAILED;
     }
 
-    if (offset > 0) {
-        munmap(guardptr, offset);
+    offset = QEMU_ALIGN_UP((uintptr_t)ptr, align) - (uintptr_t)ptr;
+
+    ret = mprotect(ptr, offset, PROT_NONE);
+    if (ret < 0) {
+        munmap(ptr, total);
     }
 
-    /*
-     * Leave a single PROT_NONE page allocated after the RAM block, to serve as
-     * a guard page guarding against potential buffer overflows.
-     */
     total -= offset;
     if (total > size + pagesize) {
-        munmap(ptr + size + pagesize, total - size - pagesize);
+        mprotect(ptr + offset + size, pagesize, PROT_NONE);
     }
 
-    return ptr;
+    return ptr + offset;
 }
 
 void qemu_ram_munmap(int fd, void *ptr, size_t size)
-- 
2.26.2

